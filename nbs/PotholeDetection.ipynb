{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pothole Detection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJxOOSMgWsvx",
    "tags": []
   },
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "2yEQxJhkTOfw",
    "outputId": "ebfeef52-19a6-4ac2-8a57-adcfd3aa876b"
   },
   "outputs": [],
   "source": [
    "! pip install -q kaggle\n",
    "# kaggle.json\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WpFMbpPRTs2f"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WXJDAlx6Txc8"
   },
   "outputs": [],
   "source": [
    "!rm -rf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSFjo8uhU28L",
    "outputId": "24534473-71a5-4ba4-e5ee-09ac5801efc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-05 07:55:33--  https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5bwfg4v4cd-1.zip\n",
      "Resolving prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)... 52.92.0.210, 52.92.19.202, 3.5.70.120, ...\n",
      "Connecting to prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com)|52.92.0.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1025803823 (978M) [application/zip]\n",
      "Saving to: ‘video_dataset.zip’\n",
      "\n",
      "video_dataset.zip   100%[===================>] 978.28M  24.0MB/s    in 40s     \n",
      "\n",
      "2024-06-05 07:56:14 (24.4 MB/s) - ‘video_dataset.zip’ saved [1025803823/1025803823]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the Video Dataset\n",
    "!wget https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5bwfg4v4cd-1.zip -O video_dataset.zip\n",
    "!unzip -q video_dataset.zip -d /content/video_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PT-6NLZU3r6",
    "outputId": "d603e646-5a31-4a1a-fde9-885b0f6db1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/farzadnekouei/pothole-image-segmentation-dataset\n",
      "License(s): Apache 2.0\n",
      "Downloading pothole-image-segmentation-dataset.zip to /content\n",
      " 81% 48.0M/59.3M [00:00<00:00, 122MB/s] \n",
      "100% 59.3M/59.3M [00:00<00:00, 82.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the Pothole Image Segmentation Dataset from Kaggle\n",
    "!kaggle datasets download farzadnekouei/pothole-image-segmentation-dataset\n",
    "!unzip -q pothole-image-segmentation-dataset.zip -d /content/pothole_image_segmentation_dataset\n",
    "!unzip -q /content/video_dataset/Pothole\\ Videos/pothole_video.zip -d /content/video_dataset/Pothole\\ Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMlXVFsYU5Da",
    "outputId": "262c5c66-5d4f-4f45-cbf1-09c3895c7d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/atulyakumar98/pothole-detection-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading pothole-detection-dataset.zip to /content\n",
      " 98% 191M/194M [00:01<00:00, 123MB/s]\n",
      "100% 194M/194M [00:01<00:00, 114MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the Pothole Detection Dataset from Kaggle\n",
    "!kaggle datasets download atulyakumar98/pothole-detection-dataset\n",
    "!unzip -q pothole-detection-dataset.zip -d /content/pothole_detection_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWKido7xU6O8",
    "outputId": "1da624c9-1648-4016-81b3-a96550e2028b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train  val\n",
      "data.yaml  README.dataset.txt  README.roboflow.txt  sample_video.mp4  train  valid\n",
      "normal\tpotholes\n"
     ]
    }
   ],
   "source": [
    "!ls /content/video_dataset/Pothole\\ Videos/pothole_video\n",
    "!ls /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8\n",
    "!ls /content/pothole_detection_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnmYwtjbWyc7"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lkwFot9MW7Fj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U ultralytics==8.0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KnTYyFopW3Ob"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLl8rOGVXNoi"
   },
   "source": [
    "### Video Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7WMkz9szXDIJ"
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, mask_dir):\n",
    "        self.video_files = sorted(os.listdir(video_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "        self.video_dir = video_dir\n",
    "        self.mask_dir = mask_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        mask = cv2.VideoCapture(mask_path)\n",
    "\n",
    "        frames = []\n",
    "        mask_frames = []\n",
    "        while(video.isOpened()):\n",
    "            ret, frame = video.read()\n",
    "            ret_mask, mask_frame = mask.read()\n",
    "            if ret and ret_mask:\n",
    "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                mask_frames.append(mask_frame)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        video.release()\n",
    "        mask.release()\n",
    "\n",
    "        return np.array(frames), np.array(mask_frames)\n",
    "\n",
    "def create_video_dataloader(video_dir, mask_dir, batch_size=4):\n",
    "    dataset = VideoDataset(video_dir, mask_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "train_video_dir = '/content/video_dataset/Pothole Videos/pothole_video/train/rgb'\n",
    "train_mask_dir = '/content/video_dataset/Pothole Videos/pothole_video/train/mask'\n",
    "\n",
    "val_video_dir = '/content/video_dataset/Pothole Videos/pothole_video/val/rgb'\n",
    "val_mask_dir = '/content/video_dataset/Pothole Videos/pothole_video/val/mask'\n",
    "\n",
    "test_video_dir = '/content/video_dataset/Pothole Videos/pothole_video/test/rgb'\n",
    "test_mask_dir = '/content/video_dataset/Pothole Videos/pothole_video/test/mask'\n",
    "\n",
    "train_dataloader = create_video_dataloader(train_video_dir, train_mask_dir)\n",
    "val_dataloader = create_video_dataloader(val_video_dir, val_mask_dir)\n",
    "test_dataloader = create_video_dataloader(test_video_dir, test_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmkwfJuVXPpv"
   },
   "source": [
    "### Pothole Image Segmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMie61fBXMeE",
    "outputId": "f61d16ec-7caf-4744-994b-1ce8530e3a2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt to yolov8n-seg.pt...\n",
      "100%|██████████| 6.73M/6.73M [00:00<00:00, 118MB/s]\n",
      "New https://pypi.org/project/ultralytics/8.2.28 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.58 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=42, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 25.9MB/s]\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 126MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "WARNING ⚠️ NMS time limit 0.550s exceeded\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/train/labels... 720 images, 0 backgrounds, 0 corrupt: 100%|██████████| 720/720 [00:01<00:00, 477.54it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/valid/labels... 60 images, 0 backgrounds, 0 corrupt: 100%|██████████| 60/60 [00:00<00:00, 423.85it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/valid/labels.cache\n",
      "Plotting labels to runs/segment/train/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      3.05G      1.567      3.545      3.262      1.565         23        640: 100%|██████████| 45/45 [00:28<00:00,  1.58it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "                   all         60        201     0.0085      0.761      0.124     0.0594    0.00828      0.741      0.119      0.056\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      3.47G      1.411      2.557      2.232       1.44         35        640: 100%|██████████| 45/45 [00:21<00:00,  2.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "                   all         60        201      0.725      0.302      0.488      0.257      0.747      0.303      0.482      0.231\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      3.47G      1.402      2.373      1.895      1.405         34        640: 100%|██████████| 45/45 [00:19<00:00,  2.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "                   all         60        201      0.557      0.338      0.382      0.199      0.547      0.313      0.361      0.179\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      3.47G      1.398      2.333      1.738      1.388         34        640: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
      "                   all         60        201      0.599      0.453      0.496      0.252      0.601      0.448      0.487      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      3.47G      1.353      2.213      1.645      1.368         29        640: 100%|██████████| 45/45 [00:20<00:00,  2.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
      "                   all         60        201      0.616      0.662       0.63      0.351      0.628      0.667       0.63      0.337\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      3.47G      1.353       2.15      1.544      1.344         50        640: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n",
      "                   all         60        201       0.58      0.483      0.541       0.25      0.564      0.473      0.482      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      3.47G      1.318      2.065      1.467      1.343         21        640: 100%|██████████| 45/45 [00:20<00:00,  2.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "                   all         60        201      0.627      0.716       0.66      0.355      0.628      0.716      0.665      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      3.47G      1.274      1.989      1.377      1.296         37        640: 100%|██████████| 45/45 [00:20<00:00,  2.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "                   all         60        201      0.564      0.637      0.627      0.341      0.568      0.641      0.635      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      3.47G      1.237      1.893      1.279      1.268         33        640: 100%|██████████| 45/45 [00:23<00:00,  1.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "                   all         60        201      0.698      0.617      0.667      0.379      0.678      0.667      0.697      0.365\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      3.47G      1.168      1.784      1.227      1.212         40        640: 100%|██████████| 45/45 [00:23<00:00,  1.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.85s/it]\n",
      "                   all         60        201      0.658      0.757      0.761      0.442      0.662      0.759      0.754      0.416\n",
      "\n",
      "10 epochs completed in 0.071 hours.\n",
      "Optimizer stripped from runs/segment/train/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from runs/segment/train/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating runs/segment/train/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.75s/it]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]\n",
      "                   all         60        201      0.664      0.766      0.764      0.444      0.664      0.766      0.754      0.417\n",
      "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 4.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "segmentation_dataset_path = '/content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8'\n",
    "\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "yaml_path = os.path.join(segmentation_dataset_path, 'data.yaml')\n",
    "\n",
    "model.train(data=yaml_path, epochs=10, imgsz=640, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQGsd9pMYOpj"
   },
   "source": [
    "### Pothole Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1Le8cLUlYOMo"
   },
   "outputs": [],
   "source": [
    "class PotholeDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label in ['normal', 'potholes']:\n",
    "            folder = os.path.join(root_dir, label)\n",
    "            for img_file in os.listdir(folder):\n",
    "                self.image_files.append(os.path.join(folder, img_file))\n",
    "                self.labels.append(0 if label == 'normal' else 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "detection_dataset_path = '/content/pothole_detection_dataset'\n",
    "detection_dataset = PotholeDetectionDataset(detection_dataset_path, transform=transform)\n",
    "detection_dataloader = DataLoader(detection_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_b9qCpaYScs"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oA2NEsMtYIon",
    "outputId": "6a85e3e6-2b02-4476-abc5-a9cb6513a860"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.28 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.58 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=/content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=42, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/train2', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/train/labels.cache... 720 images, 0 backgrounds, 0 corrupt: 100%|██████████| 720/720 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/pothole_image_segmentation_dataset/Pothole_Segmentation_YOLOv8/valid/labels.cache... 60 images, 0 backgrounds, 0 corrupt: 100%|██████████| 60/60 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "       1/10      2.95G      1.135      1.722      1.102      1.195         23        640: 100%|██████████| 45/45 [00:23<00:00,  1.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "                   all         60        201      0.653      0.731      0.733      0.424      0.649      0.726      0.725       0.41\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      3.36G      1.139      1.713      1.081      1.192         35        640: 100%|██████████| 45/45 [00:22<00:00,  2.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "                   all         60        201      0.705      0.713      0.696      0.385      0.704      0.697      0.693      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      3.36G      1.178      1.747      1.123      1.215         34        640: 100%|██████████| 45/45 [00:19<00:00,  2.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n",
      "                   all         60        201       0.63      0.677      0.689      0.373      0.619      0.677      0.671      0.344\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      3.36G      1.169      1.769      1.117       1.22         34        640: 100%|██████████| 45/45 [00:22<00:00,  2.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "                   all         60        201      0.644      0.602      0.556      0.318      0.655      0.612      0.575      0.297\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      3.36G       1.15      1.753       1.07      1.205         29        640: 100%|██████████| 45/45 [00:20<00:00,  2.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "                   all         60        201      0.727      0.527      0.612      0.348      0.754      0.547      0.636      0.327\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      3.36G      1.164      1.786      1.085      1.208         50        640: 100%|██████████| 45/45 [00:21<00:00,  2.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "                   all         60        201      0.686      0.627      0.649      0.357      0.697      0.641      0.665      0.351\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      3.36G       1.12      1.694      1.036      1.195         21        640: 100%|██████████| 45/45 [00:19<00:00,  2.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "                   all         60        201      0.692      0.642      0.671      0.387      0.704      0.652      0.683      0.376\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      3.36G      1.116      1.665     0.9883      1.178         37        640: 100%|██████████| 45/45 [00:23<00:00,  1.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
      "                   all         60        201      0.783      0.542      0.711       0.41      0.765      0.572      0.714       0.41\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      3.36G      1.082      1.611     0.9206      1.151         33        640: 100%|██████████| 45/45 [00:19<00:00,  2.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n",
      "                   all         60        201      0.682      0.701      0.695        0.4      0.688      0.701      0.706      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      3.36G      1.039      1.536     0.9006      1.119         40        640: 100%|██████████| 45/45 [00:21<00:00,  2.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n",
      "                   all         60        201       0.69      0.672       0.73      0.442      0.702      0.682      0.734       0.42\n",
      "\n",
      "10 epochs completed in 0.070 hours.\n",
      "Optimizer stripped from runs/segment/train2/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from runs/segment/train2/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating runs/segment/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.58 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients, 12.0 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]\n",
      "                   all         60        201      0.679      0.694       0.73      0.443      0.702      0.682      0.734       0.42\n",
      "Speed: 0.2ms preprocess, 3.5ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train2\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'YOLO' object has no attribute 'train_step'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1780ed4d1afe>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Perform combined training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mcombined_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-1780ed4d1afe>\u001b[0m in \u001b[0;36mcombined_training\u001b[0;34m(model, train_dataloader, val_dataloader, detection_dataloader, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvideo_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Train on image data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'train_step'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    "
     ]
    }
   ],
   "source": [
    "def combined_training(model, train_dataloader, val_dataloader, detection_dataloader, epochs=10):\n",
    "    best_loss = float('inf')\n",
    "    best_model_path = '/content/best_pothole_detection_model.pt'\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Train on video data\n",
    "        for video_batch in train_dataloader:\n",
    "            videos, masks = video_batch\n",
    "            model.train_step(videos, masks)\n",
    "\n",
    "        # Train on image data\n",
    "        for image_batch in val_dataloader:\n",
    "            images, masks = image_batch\n",
    "            model.train_step(images, masks)\n",
    "\n",
    "        # Train on detection data\n",
    "        for detection_batch in detection_dataloader:\n",
    "            images, labels = detection_batch\n",
    "            model.train_step(images, labels)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Validate on video data\n",
    "            for val_video_batch in val_dataloader:\n",
    "                val_videos, val_masks = val_video_batch\n",
    "                val_loss += model.val_step(val_videos, val_masks)\n",
    "\n",
    "            # Validate on image data\n",
    "            for val_image_batch in val_dataloader:\n",
    "                val_images, val_masks = val_image_batch\n",
    "                val_loss += model.val_step(val_images, val_masks)\n",
    "\n",
    "            # Validate on detection data\n",
    "            for val_detection_batch in detection_dataloader:\n",
    "                val_images, val_labels = val_detection_batch\n",
    "                val_loss += model.val_step(val_images, val_labels)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            model.save(best_model_path)\n",
    "\n",
    "# Perform combined training\n",
    "combined_training(model, train_dataloader, val_dataloader, detection_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXgHJLygZuFO"
   },
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).astype(np.uint8)\n",
    "    intersection = np.logical_and(pred, target)\n",
    "    union = np.logical_or(pred, target)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score\n",
    "\n",
    "def calculate_pixel_accuracy(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).astype(np.uint8)\n",
    "    correct = np.sum(pred == target)\n",
    "    total = target.size\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNlKeNnuZudg"
   },
   "outputs": [],
   "source": [
    "class PotholeDetectionEvaluator:\n",
    "    def __init__(self, model, test_dataloader):\n",
    "        self.model = model\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def evaluate(self):\n",
    "        total_iou = 0\n",
    "        total_accuracy = 0\n",
    "        total_videos = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_batch in self.test_dataloader:\n",
    "                test_videos, test_masks = test_batch\n",
    "                for video, mask in zip(test_videos, test_masks):\n",
    "                    predictions = self.model.predict(video)\n",
    "                    iou_scores = []\n",
    "                    accuracies = []\n",
    "\n",
    "                    for pred_frame, mask_frame in zip(predictions, mask):\n",
    "                        iou = calculate_iou(pred_frame, mask_frame)\n",
    "                        accuracy = calculate_pixel_accuracy(pred_frame, mask_frame)\n",
    "                        iou_scores.append(iou)\n",
    "                        accuracies.append(accuracy)\n",
    "\n",
    "                    avg_iou = np.mean(iou_scores)\n",
    "                    avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "                    total_iou += avg_iou\n",
    "                    total_accuracy += avg_accuracy\n",
    "                    total_videos += 1\n",
    "\n",
    "        avg_iou = total_iou / total_videos\n",
    "        avg_accuracy = total_accuracy / total_videos\n",
    "        print(f\"Average IoU: {avg_iou:.4f}\")\n",
    "        print(f\"Average Pixel Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "evaluator = PotholeDetectionEvaluator(best_model, test_dataloader)\n",
    "\n",
    "evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
